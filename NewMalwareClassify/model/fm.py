import tensorflow as tf
from sklearn.model_selection import train_test_split
K = tf.keras.backend

from data_loader import (vectorization_main, vectorization_andmal2017, vectorization_mix)
from metrics import precision, recall, f1_score as f1
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer

from sklearn.model_selection import GridSearchCV
from keras.wrappers import scikit_learn

class MyLayer(tf.keras.layers.Layer):
    def __init__(self, input_dim, k=30, **kwargs):
        self.input_dim = input_dim
        self.k = k
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(self.input_dim, self.k),
                                      initializer='glorot_uniform',
                                      trainable=True)
        super(MyLayer, self).build(input_shape)

    def call(self, x):
        a = K.pow(K.dot(x,self.kernel), 2)
        b = K.dot(K.pow(x, 2), K.pow(self.kernel, 2))
        return 0.5 * K.sum(a-b, 1, keepdims=True)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.k)

def FM(k=30):

    feature_dim = 808
    inputs = tf.keras.Input((feature_dim,))
    liner = tf.keras.layers.Dense(units=1, 
                                  bias_regularizer=tf.keras.regularizers.l2(0.01),
                                  kernel_regularizer=tf.keras.regularizers.l1(0.02),
                                  )(inputs)
    cross = MyLayer(feature_dim, k)(inputs)
    add = tf.keras.layers.Add()([liner, cross])
    predictions = tf.keras.layers.Activation('sigmoid')(add)
    model = tf.keras.Model(inputs=inputs, outputs=predictions)
    model.compile(loss='binary_crossentropy',
                  optimizer=tf.train.AdamOptimizer(0.001),
                  metrics=['accuracy', 'binary_accuracy', precision, recall, f1])
    model.summary()
    return model

def train():
    fm = FM(30)
    #X_train, X_test, y_train, y_test = vectorization_main()
    X_train, X_test, y_train, y_test = vectorization_andmal2017()
    #X_train, X_test, y_train, y_test = vectorization_mix()

    print(X_train.shape)
    fm.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))
    y_pred = fm.predict(X_test)
    y_pred = np.array([1 if y[0] > 0.5 else 0 for y in y_pred])

    return accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)

def search_param():

    model = scikit_learn.KerasClassifier(build_fn=FM, epochs=50, batch_size=32, verbose=0)
    k = range(10, 200, 10)
    param_grid = dict(k=k)
    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)#, scoring = make_scorer(f1_score, greater_is_better = False))
    X_train, X_test, y_train, y_test = vectorization_main()
    grid_result = grid.fit(X_train, y_train)
    print('Best: {} using {}'.format(grid_result.best_score_, grid_result.best_params_))

    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']


    for mean, std, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, std, param))

def evaluate():
    result = []
    try_cnt = 30
    for i in range(try_cnt):
        result.append(train())
    acc = sum([result[i][0] for i in range(try_cnt)])/try_cnt
    pre = sum([result[i][1] for i in range(try_cnt)])/try_cnt
    rec = sum([result[i][2] for i in range(try_cnt)])/try_cnt
    f1 = sum([result[i][3] for i in range(try_cnt)])/try_cnt
    print(acc, pre, rec, f1)
    '''
    derbin0 vs benign:
        #FM: 0.9704839809134284 0.9627679141270874 0.9378616552582428 0.9499759209984381
        #0.9716087252897067 0.9704489335076435 0.9346745448651592 0.9521058071703464
        #DNN:0.9640422631220174 0.9388329089180775 0.9416478345079741 0.9400349661664494
        #RF: 0.9604635310156779 0.966719876336477 0.8997544401225606 0.9318782426822927
        #SVM:0.9386843899113841 0.8918344238355598 0.909683549032539 0.9002710534363695
    andmal2017 vs benign
        #FM: 0.9756847871892123 0.9694456220560569 0.8456739476512424 0.9028093023584215
        #DNN: 0.9705857564264645 0.9178673333513797 0.8598576618363882 0.8874660565911375
            0.9686894226717235 0.9011524080684262 0.8638187452545615 0.881544310403083
        #RF: 0.9562579013906449 0.9968358822463259 0.6864694142982386 0.8122672065800708
        #SVM  0.8908975979772441 1.0 0.20902194050893483 0.3446623585572391
    derbin train, test andmal
        #SVM 0.773244225210039 0.7550215689803214 0.5050139275766017 0.6049720597640941
        #RF 0.7571802458797187 0.9049416771940102 0.32720519962859795 0.4801879084276421
        #FM 0.7915437573643708 0.9216230549837917 0.43203342618384405 0.5880398208750166
        #DNN 0.7893863759476966 0.8711564418688794 0.455803156917363 0.597970712633963
    andmal2017_split_key vs benign
        #SVM: 0.8036992169357541 0.9333333333333333 0.09259566083468977 0.1649866067758636
        #RF: 0.8230196010682658 0.9901118214430906 0.18768062409091443 0.31069352457584737
        #FM: k=50 0.8869907518794992 0.9749164835172416 0.4779022005173427 0.6339536623525508
        k=30 0.8853013098346357 0.9687849856806139 0.490242001924721 0.6446562487166224
        #DNN: 0.8910236077049818 0.9351603969799547 0.5370770503740407 0.6756542646963335
    '''

if __name__ == '__main__':
    #train()
    evaluate()
    #search_param()